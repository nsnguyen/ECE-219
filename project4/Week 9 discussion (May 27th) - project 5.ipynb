{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to start processing the data?\n",
    "    - Online Json Viewer - useful webpage to play with the data: http://jsonviewer.stack.hu\n",
    "    - Read the text file, read it line by line (we go through the code)\n",
    "    - Pandas Dataframe: easily play with the time, and aggregate the columns.\n",
    "    - Reminder: You can store the processed data/features in pickle objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/ECE219_tweet_data/tweets_#gohawks.txt',\n",
       " '../data/ECE219_tweet_data/tweets_#gopatriots.txt',\n",
       " '../data/ECE219_tweet_data/tweets_#nfl.txt',\n",
       " '../data/ECE219_tweet_data/tweets_#patriots.txt',\n",
       " '../data/ECE219_tweet_data/tweets_#sb49.txt',\n",
       " '../data/ECE219_tweet_data/tweets_#superbowl.txt']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "glob function: finds all the pathnames matching a specific pattern.\n",
    "more info: https://docs.python.org/2/library/glob.html\n",
    "'''\n",
    "import glob\n",
    "\n",
    "datafilenames = [i for i in glob.iglob('../data/ECE219_tweet_data/*.txt')]\n",
    "#datafilenames = [i for i in glob.iglob('../data/ECE219_tweet_data/*gopatriots*.txt')]\n",
    "datafilenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/ECE219_tweet_data/tweets_#gohawks.txt.pkl.gz\tsaved.\n",
      "../data/ECE219_tweet_data/tweets_#gopatriots.txt.pkl.gz\tsaved.\n",
      "../data/ECE219_tweet_data/tweets_#nfl.txt.pkl.gz\tsaved.\n",
      "../data/ECE219_tweet_data/tweets_#patriots.txt.pkl.gz\tsaved.\n",
      "../data/ECE219_tweet_data/tweets_#sb49.txt.pkl.gz\tsaved.\n",
      "../data/ECE219_tweet_data/tweets_#superbowl.txt.pkl.gz\tsaved.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Instead of loading the entire file, you can read the files, line by line and keep the information you need.\n",
    "Store the processed data in pickle objects.\n",
    "'''\n",
    "import json\n",
    "import pickle, gzip\n",
    "\n",
    "OVERWRITE_PICKLE_OBJECTS = True\n",
    "\n",
    "def extract_feature_from_tweet(tweet_dict,feature_name):\n",
    "    if feature_name == \"time\": \n",
    "        return tweet_dict[\"citation_date\"]\n",
    "    if feature_name == \"time_ori\":\n",
    "        return tweet_dict[\"firstpost_date\"]\n",
    "    if feature_name == \"re_count\":\n",
    "        return tweet_dict[\"metrics\"][\"citations\"][\"total\"]\n",
    "    if feature_name == \"fo_count\":\n",
    "        return tweet_dict[\"author\"][\"followers\"]\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "feature_names = [\"time\", \"time_ori\", \"re_count\", \"fo_count\"]\n",
    "\n",
    "\n",
    "\n",
    "for fn in datafilenames:\n",
    "    if os.path.isfile(fn+'.pkl.gz') and not OVERWRITE_PICKLE_OBJECTS:\n",
    "        print(fn+'.pkl.gz'+'\\texists.')\n",
    "        '''\n",
    "        # To also load the pickle file \n",
    "        with gzip.open(fn+'.pkl.gz', 'rb') as f:\n",
    "            tweets_features = pickle.load(f)\n",
    "        print(fn+'.pkl.gz'+'\\tloaded.')\n",
    "        '''\n",
    "    else:    \n",
    "        tweets_features = dict() \n",
    "        # {\"time\":[444,444,], \"re_count\":[23,34]}\n",
    "        # the other way: [{\"time\": 44, \"re_count\":23}]\n",
    "        for f in feature_names:\n",
    "            tweets_features[f] = []\n",
    "        \n",
    "        with open(fn,'rb') as d:\n",
    "            for ind, line in enumerate(d):\n",
    "                if ind > 1000:\n",
    "                    break        \n",
    "                tweet_dict = json.loads(line) \n",
    "                for f in feature_names:\n",
    "                    feature_value = extract_feature_from_tweet(tweet_dict, f)\n",
    "                    tweets_features[f].append(feature_value)\n",
    "        with gzip.open(fn+'.pkl.gz','wb') as p:\n",
    "            pickle.dump(tweets_features,p,protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        print(fn+'.pkl.gz'+'\\tsaved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'re_count': [2,\n",
       "  15,\n",
       "  2,\n",
       "  2,\n",
       "  7,\n",
       "  34,\n",
       "  3,\n",
       "  10,\n",
       "  6,\n",
       "  11,\n",
       "  3,\n",
       "  3,\n",
       "  2,\n",
       "  10,\n",
       "  4,\n",
       "  6,\n",
       "  4,\n",
       "  4,\n",
       "  2,\n",
       "  33,\n",
       "  2,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  10,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  6,\n",
       "  5,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  2,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  6,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  20,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  10,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  7,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  13,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  7,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  5,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  7,\n",
       "  2,\n",
       "  1,\n",
       "  7,\n",
       "  3,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  28,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  6,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  6,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  11,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  5,\n",
       "  2,\n",
       "  2,\n",
       "  6,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  13,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  4,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  1,\n",
       "  4,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  5,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  8,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  6,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  7,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  5,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  11,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  17,\n",
       "  1,\n",
       "  1,\n",
       "  5,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  5,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  5,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  16,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  21,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  5,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  20,\n",
       "  37,\n",
       "  ...]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "An alternative solution.\n",
    "'''\n",
    "\n",
    "def dict_xpath_get(mydict, path):\n",
    "    elem = mydict\n",
    "    try:\n",
    "        for x in path.strip(\"/\").split(\"/\"):\n",
    "            try:\n",
    "                x = int(x)\n",
    "                elem = elem[x]\n",
    "            except ValueError:\n",
    "                elem = elem.get(x)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return elem\n",
    "\n",
    "def feature_extract(tweet_dict, feature_selectors):\n",
    "    extracted_dict = dict()\n",
    "    for f in feature_selectors:\n",
    "        selector = feature_selectors[f]\n",
    "        extracted_dict[f] = dict_xpath_get(tweet_dict, selector)\n",
    "    return extracted_dict\n",
    "\n",
    "\n",
    "feature_selectors = {\n",
    "    'time'         : '/citation_date',\n",
    "    'time_ori'     : '/firstpost_date',\n",
    "    're_count'     : '/metrics/citations/total',\n",
    "    'fo_count'     : '/author/followers',\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "for fn in datafilenames:\n",
    "    if os.path.isfile(fn+'.pkl.gz') and not OVERWRITE_PICKLE_OBJECTS:\n",
    "        print(fn+'.pkl.gz'+'\\texists.')\n",
    "#         with gzip.open(fn+'.pkl.gz', 'rb') as f:\n",
    "#             tweets_features = pickle.load(f)\n",
    "#         print(fn+'.pkl.gz'+'\\tloaded.')\n",
    "    else:\n",
    "        tweets_features = dict()\n",
    "        for f in feature_selectors:\n",
    "            tweets_features[f] = []\n",
    "\n",
    "        with open(fn,'rb') as d:\n",
    "            for ind, line in enumerate(d):\n",
    "                if ind > 1000:\n",
    "                    break\n",
    "                tweet_dict = json.loads(line)\n",
    "                feature_dict = feature_extract(tweet_dict, feature_selectors)\n",
    "                for f in feature_dict:\n",
    "                    tweets_features[f].append(feature_dict[f])\n",
    "        with gzip.open(fn+'.pkl.gz','wb') as p:\n",
    "            pickle.dump(tweets_features,p,protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        print(fn+'.pkl.gz'+'\\tsaved.')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "based_dir = '../data/ECE219_tweet_data/'\n",
    "\n",
    "for hashtag in ['superbowl', 'nfl', 'gohawks', 'gopatriots', 'patriots', 'sb49']:\n",
    "    with gzip.open(based_dir + 'tweets_#' + hashtag +'.txt.pkl.gz', 'rb') as f:\n",
    "        features_superbowl = pickle.load(f)\n",
    "    pd_df = pd.DataFrame(features_superbowl)\n",
    "    '''\n",
    "    Start the analysis ...\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fo_count</th>\n",
       "      <th>re_count</th>\n",
       "      <th>time</th>\n",
       "      <th>time_ori</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59</td>\n",
       "      <td>2</td>\n",
       "      <td>1421238675</td>\n",
       "      <td>1421238675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>1421244058</td>\n",
       "      <td>1421244058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>1421246898</td>\n",
       "      <td>1421246898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>533</td>\n",
       "      <td>2</td>\n",
       "      <td>1421249217</td>\n",
       "      <td>1421249217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>260</td>\n",
       "      <td>1</td>\n",
       "      <td>1421249288</td>\n",
       "      <td>1421249288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fo_count  re_count        time    time_ori\n",
       "0        59         2  1421238675  1421238675\n",
       "1        21         1  1421244058  1421244058\n",
       "2        53         1  1421246898  1421246898\n",
       "3       533         2  1421249217  1421249217\n",
       "4       260         1  1421249288  1421249288"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd_df['citetime'] = pd.to_datetime(pd_df['time'], unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fo_count</th>\n",
       "      <th>re_count</th>\n",
       "      <th>time</th>\n",
       "      <th>time_ori</th>\n",
       "      <th>citetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59</td>\n",
       "      <td>2</td>\n",
       "      <td>1421238675</td>\n",
       "      <td>1421238675</td>\n",
       "      <td>2015-01-14 12:31:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>1421244058</td>\n",
       "      <td>1421244058</td>\n",
       "      <td>2015-01-14 14:00:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>1421246898</td>\n",
       "      <td>1421246898</td>\n",
       "      <td>2015-01-14 14:48:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>533</td>\n",
       "      <td>2</td>\n",
       "      <td>1421249217</td>\n",
       "      <td>1421249217</td>\n",
       "      <td>2015-01-14 15:26:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>260</td>\n",
       "      <td>1</td>\n",
       "      <td>1421249288</td>\n",
       "      <td>1421249288</td>\n",
       "      <td>2015-01-14 15:28:08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fo_count  re_count        time    time_ori            citetime\n",
       "0        59         2  1421238675  1421238675 2015-01-14 12:31:15\n",
       "1        21         1  1421244058  1421244058 2015-01-14 14:00:58\n",
       "2        53         1  1421246898  1421246898 2015-01-14 14:48:18\n",
       "3       533         2  1421249217  1421249217 2015-01-14 15:26:57\n",
       "4       260         1  1421249288  1421249288 2015-01-14 15:28:08"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytz\n",
    "utc_tz = pytz.UTC\n",
    "pst_tz = pytz.timezone('America/Los_Angeles')\n",
    "\n",
    "pd_df['citetime_pst'] = pd.to_datetime(pd_df['time'], unit='s').apply(lambda x: x.tz_localize(utc_tz).astimezone(pst_tz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fo_count</th>\n",
       "      <th>re_count</th>\n",
       "      <th>time</th>\n",
       "      <th>time_ori</th>\n",
       "      <th>citetime</th>\n",
       "      <th>citetime_pst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59</td>\n",
       "      <td>2</td>\n",
       "      <td>1421238675</td>\n",
       "      <td>1421238675</td>\n",
       "      <td>2015-01-14 12:31:15</td>\n",
       "      <td>2015-01-14 04:31:15-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>1421244058</td>\n",
       "      <td>1421244058</td>\n",
       "      <td>2015-01-14 14:00:58</td>\n",
       "      <td>2015-01-14 06:00:58-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>1421246898</td>\n",
       "      <td>1421246898</td>\n",
       "      <td>2015-01-14 14:48:18</td>\n",
       "      <td>2015-01-14 06:48:18-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>533</td>\n",
       "      <td>2</td>\n",
       "      <td>1421249217</td>\n",
       "      <td>1421249217</td>\n",
       "      <td>2015-01-14 15:26:57</td>\n",
       "      <td>2015-01-14 07:26:57-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>260</td>\n",
       "      <td>1</td>\n",
       "      <td>1421249288</td>\n",
       "      <td>1421249288</td>\n",
       "      <td>2015-01-14 15:28:08</td>\n",
       "      <td>2015-01-14 07:28:08-08:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fo_count  re_count        time    time_ori            citetime  \\\n",
       "0        59         2  1421238675  1421238675 2015-01-14 12:31:15   \n",
       "1        21         1  1421244058  1421244058 2015-01-14 14:00:58   \n",
       "2        53         1  1421246898  1421246898 2015-01-14 14:48:18   \n",
       "3       533         2  1421249217  1421249217 2015-01-14 15:26:57   \n",
       "4       260         1  1421249288  1421249288 2015-01-14 15:28:08   \n",
       "\n",
       "               citetime_pst  \n",
       "0 2015-01-14 04:31:15-08:00  \n",
       "1 2015-01-14 06:00:58-08:00  \n",
       "2 2015-01-14 06:48:18-08:00  \n",
       "3 2015-01-14 07:26:57-08:00  \n",
       "4 2015-01-14 07:28:08-08:00  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_windowed = pd_df.groupby(pd.Grouper(key='citetime_pst', freq=\"60Min\")).agg({'re_count': 'sum'})\n",
    "# read more about the agg function: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.groupby.DataFrameGroupBy.agg.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>re_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>citetime_pst</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-14 04:00:00-08:00</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-14 05:00:00-08:00</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-14 06:00:00-08:00</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-14 07:00:00-08:00</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-14 08:00:00-08:00</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           re_count\n",
       "citetime_pst                       \n",
       "2015-01-14 04:00:00-08:00         2\n",
       "2015-01-14 05:00:00-08:00       NaN\n",
       "2015-01-14 06:00:00-08:00         2\n",
       "2015-01-14 07:00:00-08:00         3\n",
       "2015-01-14 08:00:00-08:00         2"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_windowed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>re_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>citetime_pst</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-14 04:00:00-08:00</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-14 05:00:00-08:00</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-14 06:00:00-08:00</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-14 07:00:00-08:00</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-14 08:00:00-08:00</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           re_count\n",
       "citetime_pst                       \n",
       "2015-01-14 04:00:00-08:00         2\n",
       "2015-01-14 05:00:00-08:00         0\n",
       "2015-01-14 06:00:00-08:00         2\n",
       "2015-01-14 07:00:00-08:00         3\n",
       "2015-01-14 08:00:00-08:00         2"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_windowed.fillna(value=0).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Innovation Part\n",
    "    - Look through the literature, and list some potential tasks for this part. Start with the paper we referred to in the spec.\n",
    "    - There are many different analysis tasks, try to be creative!\n",
    "   \n",
    "   \n",
    "\n",
    "## Useful links/materials\n",
    "    - Statsmodels -> Formulas : http://www.statsmodels.org/dev/examples/notebooks/generated/formulas.html\n",
    "        - Enables R-like regression formulas\n",
    "        - Easy to modify small feature sets. \n",
    "        - Can handle categorical features pretty well\n",
    "        - In general dataframes, and other techniques can also handle the categorical features as well. (pd.get_dummies)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
